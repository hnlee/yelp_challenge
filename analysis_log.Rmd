Analysis log
========================================================

### Loading data

* Installed `jsonlite` and `knitr` packages

```{r}
setwd('/Users/hanalee/yelp_challenge')
library(jsonlite)
```

### Quiz questions

Load data and cache for loading
```{r}
business <- stream_in(file("dataset/yelp_academic_dataset_business.json"))
checkin <- stream_in(file("dataset/yelp_academic_dataset_checkin.json"))
review <- stream_in(file("dataset/yelp_academic_dataset_review.json"))
tip <- stream_in(file("dataset/yelp_academic_dataset_tip.json"))
user <- stream_in(file("dataset/yelp_academic_dataset_user.json"))
yelp_data <- save(business, checkin, review, tip, user,
                  file="yelp_data.rda")
```

Exploratory data analysis for quiz 1
```{r}
nrow(review)
review[100,]
colnames(review)
length(which(review$stars==5))/nrow(review)

nrow(business)
colnames(business)
colnames(business$attributes)
length(which(business$attributes[,"Wi-Fi"] == "free"))/sum(!(is.na(business$attributes[,"Wi-Fi"])))

nrow(tip)
tip[1000,]

colnames(user)
colnames(user$votes)
intersect(c("Ira","Brian","Jeff","Roger"), user[which(user$votes[,"funny"] > 10000), "name"])
fisher.test(table(user$votes[,"funny"] > 1, user$fans > 1))
```

### Brainstorming

```{r}
load("yelp_data.rda")
library(dplyr)
library(reshape2)
```

I'm interested in urban planning aspects of the data set. Possible  questions for the project:
* Predict food deserts
* Predict neighborhood popularity (with locals, with tourists)
* Predict neighborhood of a business given the users who review it
* Predict priciness of a neighborhood

Pre-processing steps
* I want to create a variable to code for which metropolitan area
  * Must determine which latitude & longitude ranges are appropriate
  * Use a function to detect clusters
  * Ten cities so k-means where k=10
* Within each city, identify neighborhoods
  * Do neighborhoods overlap?
  * Infer missing values for businesses without neighborhood based on latitude and longitude
* Do I need to be more local than neighborhood?

```{r}
cities <- kmeans(select(business, latitude, longitude), 
                 centers=10, nstart=10000, iter.max=10000)
cities$size
cities$centers
length(cities$cluster)
business$metro <- as.factor(cities$cluster)

cityplot <- ggplot(data=select(business, metro, latitude, longitude)) 
cityplot + geom_point(aes(x=longitude, y=latitude, color=metro))
```

Partitioning steps
* 10% of business_id for test set?

```{r}
length(setdiff(checkin$business_id, business$business_id))
length(setdiff(tip$business_id, business$business_id))
length(setdiff(review$business_id, business$business_id))
```

Need to avoid circularity where what is being predicted is also in the predictors.

Use external data set to determine what to predict?

Predict something about a category of businesses?

Cities can be assigned based on states rather than through k-means

```{r}
levels(as.factor(business$state))
sapply(levels(as.factor(business$state)), function(x) {
  nrow(business[business$state == x,])
})
business$metro <- sapply(business$city, function(x){
  if(x == "AZ"){return("Phoenix")}
  if(x == "IL"){return("Urbana-Champaign")}
  if(x %in% c("NC", "SC")){return("Charlotte")}
  if(x == "NV"){return("Las Vegas")}
  if(x == "PA"){return("Pittsburgh")}
  if(x == "WI"){return("Madison")}
  if(x == "QC"){return("Montreal")}
  if(x == "ON"){return("Waterloo")}
  if(x %in% c("BW", "RP")){return("Karlsruhe")}
  if(x %in% c("EDH", "ELN", "FIF", "HAM", "KHL", "MLN", "NTH", "SCB", "XGL")){
    return("Edinburgh")
  } else { return(NA)}
})
```

### Data cleaning

Check for missing data in: 
  - name
  - longitude
  - latitude
  - state
  - stars
  - review_count
  - open

```{r}
colnames(business)
business_meta <- c("business_id", "name", "longitude","latitude", "state", "stars","review_count", "open")
business_flat <- business[, business_meta]
colSums(is.na(business[, business_meta]))
```

Expand out:
  - hours X
  - categories
  - attributes X
  - neighborhoods - character(0) needs to be NA

```{r}
head(business$categories, 5)
colnames(business$attributes)
colnames(business$hours)
head(business$neighborhoods, 5)
```

Cleaning up attributes
```{r}
attribute_classes <- sapply(colnames(business$attributes), function(x){ class(business$attributes[,x]) })
attributes <- as.data.frame(business$attributes[, which(attribute_classes == "logical")])

which(attribute_classes == "character")
attributes$`Alcohol` <- as.factor(business$attributes$`Alcohol`)
attributes$`Noise Level` <- as.factor(business$attributes$`Noise Level`)
attributes$`Attire` <- as.factor(business$attributes$`Attire`)
attributes$`Smoking` <- as.factor(business$attributes$`Smoking`)
attributes$`Wi-Fi` <- as.factor(business$attributes$`Wi-Fi`)
attributes$`Ages Allowed` <- as.factor(business$attributes$`Ages Allowed`)
attributes$`BYOB/Corkage` <- as.factor(business$attributes$`BYOB/Corkage`)

which(attribute_classes == "list")
attributes$`Accepts Credit Cards` <- sapply(business$attributes$`Accepts Credit Cards`, function(x){
  if(length(x) == 0) { return(NA) } else { return(x[1]) }
})

head(attributes, 5)

which(attribute_classes == "data.frame")
sapply(colnames(business$attributes$`Ambience`), function(x){ class(business$attributes$`Ambience`[,x]) })
sapply(colnames(business$attributes$`Good For`), function(x){ class(business$attributes$`Good For`[,x]) })
sapply(colnames(business$attributes$`Music`), function(x){ class(business$attributes$`Music`[,x]) })
sapply(colnames(business$attributes$`Hair Types Specialized In`), function(x){ class(business$attributes$`Hair Types Specialized In`[,x]) })
sapply(colnames(business$attributes$`Payment Types`), function(x){ class(business$attributes$`Payment Types`[,x]) })
sapply(colnames(business$attributes$`Parking`), function(x){ class(business$attributes$`Parking`[,x]) })
sapply(colnames(business$attributes$`Dietary Restrictions`), function(x){ class(business$attributes$`Dietary Restrictions`[,x]) })

ambience <- as.data.frame(business$attributes$`Ambience`)
good_for <- as.data.frame(business$attributes$`Good For`)
music <- as.data.frame(business$attributes$`Music`)
hair_types <- as.data.frame(business$attributes$`Hair Types Specialized In`)
payment <- as.data.frame(business$attributes$`Payment Types`)
parking <- as.data.frame(business$attributes$`Parking`)
dietary <- as.data.frame(business$attributes$`Dietary Restrictions`)

```

Cleaning up hours
```{r}
hours <- cbind(sapply(colnames(business$hours), function(x){
    as.numeric(substr(business$hours[,x][,"open"], 1, 2))
  }),
  sapply(colnames(business$hours), function(x){
    as.numeric(substr(business$hours[,x][,"close"], 1, 2))
  })
)
colnames(hours) <- paste(rep(colnames(business$hours), each=2), c("open","close"), sep='_')
hours <- as.data.frame(hours)
head(hours)
```

Cleaning up categories - only use top-level categories from Yelp
```{r}
top_level <- c("Active Life", "Arts & Entertainment", "Automotive", "Beauty & Spas", "Education",
  "Event Planning & Services", "Financial Services", "Food", "Health & Medical", "Home Services",
  "Hotels & Travel", "Local Flavor", "Local Services", "Mass Media", "Nightlife", "Pets", 
  "Professional Services", "Public Services & Government", "Real Estate", "Religious Organizations",
  "Restaurants", "Shopping")
sum(sapply(business$categories, function(x) { length(intersect(top_level, x)) != 1 & length(x) != 0 }))
categories <- as.data.frame(cbind(sapply(top_level, function(x) {
  sapply(business$categories, function(y) {
    x %in% y
  })
})))
```

Ignoring neighborhoods for now -- may later use along with clustering of longitude and latitude

Note that review count does not equal total number of reviews, but total number of users who left reviews.
```{r}
check <- head(business[,c("business_id", "review_count")],5)
colnames(review)
length(which(review$business_id == check$business_id[1]))
review[which(review$business_id == check$business_id[1]), "user_id"]
total_review <- sapply(business$business_id, function(x) {
  length(which(review$business_id == x))
})
```

Add in features:
  - number of tips
  - number of checkins
  - number of repeat reviews
  - number of repeat checkins
  - number of repeat tips
```{r}
total_tip <- sapply(business$business_id, function(x) {
  length(which(tip$business_id == x))
})
total_checkin <- sapply(business$business_id, function(x) {
  length(which(checkin$business_id == x))
})
repeat_review <- sapply(business$business_id, function(x) {
  review_number <- table(review[which(review$business_id == x), "user_id"])
  return(length(which(review_number > 1)))
})
repeat_tip <- sapply(business$business_id, function(x) {
  tip_number <- table(tip[which(tip$business_id == x), "user_id"])
  return(length(which(tip_number > 1)))
})
repeat_checkin <- sapply(business$business_id, function(x) {
  checkin_number <- table(checkin[which(checkin$business_id == x), "user_id"])
  return(length(which(checkin_number > 1)))
})
```

What to predict:
  - stars
  - number of reviews (total or unique?)
  - total of reviews, checkins, tips?
```{r}
total_rct <- rowSums(cbind(total_review, total_tip,  total_checkin))
```

Note that stars is actually ordered categorical (steps of 0.5) -- create a star average to predict?
```{r}
ggplot(data=as.data.frame(business[,c("business_id", "stars")])) + geom_histogram(aes(x=stars), binwidth=0.5)
mean_stars <- sapply(business$business_id, function(x) {
  return(mean(review[which(review$business_id == x), "stars"], na.rm=TRUE))
})
```
Only problem is that review data set is not all reviews are in data set...

Saving everything to reload easily
```{r}
save(business_flat,
  attributes,
  ambience,
  good_for,
  music,
  parking,
  hair_types,
  payment,
  parking,
  dietary,
  hours,
  categories,
  total_review,
  total_tip,
  total_checkin,
  repeat_review,
  repeat_tip,
  repeat_checkin,
  mean_stars,
  total_rct, file="yelp_clean.rda")
business_data <- as.data.frame(cbind(business_flat,
  attributes,
  ambience,
  good_for,
  music,
  parking,
  hair_types,
  payment,
  parking,
  dietary,
  hours,
  categories,
  total_review,
  total_tip,
  total_checkin,
  repeat_review,
  repeat_tip,
  repeat_checkin,
  mean_stars,
  total_rct))
save(business_data, file="yelp_tidy.rda")
load("yelp_clean.rda")
load("yelp_tidy.rda")
```

### Data partitioning

70-30 split into training and test data

```{r}
train_index <- createDataPartition(business$stars, p=0.7, groups=9, list=FALSE)
train <- business_data[train_index,]
test <- business_data[-train_index,]
save(train, file="yelp_train.rda")
save(test, file="yelp_test.rda")
```

Supervised learning on training data

```{r}
load("yelp_train.rda")
library(caret)
library(ggplot2)
```

Check for missing values
Check for near zero variance

```{r}
colnames(train)
nonpredictors <- c("business_id", "name", "stars", "mean_stars",
                   "total_review", "total_rct", "state")
colSums(is.na(train[,nonpredictors]))
predictors <- setdiff(colnames(train), nonpredictors)
missing <- colSums(is.na(train[,predictors]))/nrow(train)
ggplot(data = data.frame(cbind(numNA = missing))) + geom_histogram(aes(x = numNA), binwidth=0.05)
filtered_predictors <- predictors[which(missing <= 0.65)]
missing[filtered_predictors]

nzv <- nearZeroVar(train[, filtered_predictors], saveMetrics = TRUE)
nzv
which(nzv$nzv)
filtered_predictors <- filtered_predictors[-which(nzv$nzv)]
filtered_predictors
```


Check for highly correlated predictors

```{r}
corr_matrix <- cor(as.data.frame(train[, filtered_predictors]), use="pairwise.complete.obs")
correlated_predictors <- findCorrelation(corr_matrix, verbose=TRUE, exact=TRUE)
filtered_predictors[correlated_predictors]
filtered_predictors <- filtered_predictors[-correlated_predictors]
```

Plot correlations with outcome for continuous
wilcoxon-test for binary/ANOVA for multiple categories

```{r}
variable_types <- sapply(filtered_predictors, function(x) {
  class(train[, x])
})
outcome_corr <- sapply(filtered_predictors[which(variable_types == "numeric" | variable_types=="integer")], function(x) {
  cor(train$stars, train[, x], use="complete.obs")
})
outcome_corr

outcome_wilcox <- sapply(filtered_predictors[which(variable_types == "logical")], function(x){
  true_val <- which(!is.na(train[, x]) & train[, x])
  false_val <- which(!is.na(train[, x]) & !train[, x])
  return(-log(wilcox.test(train[true_val, "stars"], train[-false_val, "stars"])$p.value))
})
outcome_wilcox
```

Running models

```{r}
install.packages("randomForest")
install.packages("gbm")
install.packages("xgboost")
install.packages("kernlab")
library(randomForest)
library(gbm)
library(xgboost)
library(kernlab)
set.seed(333)
```

Setting up cross-validation for predictor choice
```{r}
control <- trainControl(method="repeatedcv", number=5, repeats=5)
```

Trying random forests
```{r}
m <- floor(length(filtered_predictors)/3)
rf_tuning <- data.frame(mtry = 1:m)
rf_fit <- train(stars ~ ., data=train[,c(filtered_predictors, "stars")],
                method="rf",
                trControl=control,
                verbose=TRUE,
                tuneGrid=rf_tuning,
                importance=TRUE)
mtry_tuning <- ggplot(rf_fit)
mtry_tuning
rf_fit
rf_imp <- varImp(rf_fit)
plot(rf_imp)
```

Notes:
- optimal mtry=three
- feature importance: 

Performance on test data:
```{r}
load("yelp_test.rda")
rf_predict <- predict(rf_fit, newdata=test[, filtered_predictors])
rf_fit$call
rf_fit_plot <- ggplot(data=data.frame(cbind(prediction=rf_predict, actual=test$stars))) + geom_point(aes(x=prediction, y=actual))
```


Trying gradient boosting
Hastie says performance usually does not improve past J ~ 6
```{r}
gbm_tuning <-  expand.grid(interaction.depth = 1:5,
                           n.trees = c(10,50,100,500,1000,5000))
gbm_fit <- train(stars ~ ., data=train[,c(filtered_predictors, "stars")],
                method="gbm",
                trControl=control,
                verbose=TRUE,
                tuneGrid=gbm_tuning,
                importance=TRUE)
param_tuning <- ggplot(gbm_fit)
gbm_fit
gbm_imp <- varImp(gbm_fit)
ggplot(gbm_imp)
```
